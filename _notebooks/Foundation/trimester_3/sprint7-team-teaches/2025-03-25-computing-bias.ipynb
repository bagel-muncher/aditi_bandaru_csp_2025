{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "toc: true\n",
    "layout: post\n",
    "title: Computing Bias\n",
    "description: Classwork and Homework for the team teach\n",
    "permalink: /_posts/Foundation/trimester_3/sprint7-team-teaches/2025-03-25-computing-bias_IPYNB_2_\n",
    "categories: [Trimester 3]\n",
    "comments: true\n",
    "type: ccc\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classwork Activity\n",
    "\n",
    "I drew a nurse. I did have bias because from my time as a hospital volunteer, the majority of nurses I have seen are women who wear scrubs and like to have their hair tied up.\n",
    "<br>\n",
    "<img src=\"{{site.baseurl}}/images/team_teaches_7/nurse.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack #1:\n",
    "Think of a real-world example where a computer system has shown bias. It could be something you’ve read about, experienced, or imagined.\n",
    "\n",
    "**Question:**\n",
    "Describe the biased system, explain what type of bias it represents (Pre-existing Social Bias, Technical Bias, or Emergent Social Bias), and suggest one way to reduce or fix the bias.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A real-world example of a biased computer system is predictive policing algorithms that disproportionately target minority communities. Some law enforcement agencies use AI-driven tools to predict where crimes are likely to occur and allocate police resources accordingly. However, studies have found that these systems often direct more policing to historically over-policed neighborhoods, leading to a cycle of increased arrests in certain communities. This represents is an exmaple of Emergent Social Bias, which develops over time as a system interacts with societal structures. The bias emerges because the algorithm relies on past crime data, which may already reflect biased policing practices. As a result, the system reinforces existing disparities rather than providing an objective analysis of crime patterns. One way to reduce or fix the bias is to ensure the algorithm is trained on diverse and unbiased datasets, incorporating factors beyond just historical crime reports. Additionally, law enforcement agencies should use human oversight and transparency measures to prevent the system from disproportionately targeting specific groups and ensure fair decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack #2:\n",
    "Bias in computing can lead to unfair outcomes, but there are ways to reduce it\n",
    "\n",
    "**Question:**\n",
    "In the financial industry, an AI system used to approve loan applications unintentionally favors male applicants over female applicants because it was trained on past loan approval data, which reflected gender biases. This is an example of Pre-existing Social Bias.\n",
    "\n",
    "Give two ways to mitigate this bias and make the system more fair.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "One way to mitigate this bias in the AI loan approval system and make it more fair is to re-train the AI model with a more balanced dataset. The system should be trained on data that is representative of all applicants, ensuring that past gender biases are not carried forward. This could involve removing gender as a factor in decision-making and using a dataset that reflects equitable lending practices. Another way to weaken this bias is to implement fairness constraints and bias audits. Financial institutions should incorporate algorithmic fairness checks to detect and correct biases in loan approval decisions. Regular audits, transparency measures, and external reviews can help ensure the AI does not systematically favor one gender over another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Hack: Understanding Bias in Computing\n",
    "\n",
    "**Question:**\n",
    "Think of a system or tool that you use every day—this could be a website, app, or device. Can you identify any bias that might exist in the way the system works?\n",
    "\n",
    "Task:\n",
    "1. Describe the system you’re thinking of.\n",
    "2. Identify the bias in that system and explain why it might be biased. (Is it Pre-existing Social Bias, Technical Bias, or Emergent Social Bias?)\n",
    "3. Propose one way to reduce or fix the bias in that system.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. Spotify is a popular music streaming service that provides users with personalized playlists, recommendations, and curated content based on listening habits. The platform uses AI algorithms to suggest songs and artists, creating a unique experience for each user.\n",
    "\n",
    "2. Spotify’s recommendation algorithm can exhibit Emergent Social Bias by reinforcing existing preferences and limiting musical diversity. Since the system suggests songs based on past listening history, users may get stuck in a \"filter bubble,\" where they are repeatedly recommended similar genres or artists while being less exposed to new or diverse music. This can lead to underrepresentation of lesser-known artists, particularly from niche genres or non-Western music scenes.\n",
    "\n",
    "3. To reduce this bias, Spotify could introduce more intentional diversity in its recommendation system by occasionally suggesting songs from different genres, cultures, or underrepresented artists. This could be done through an optional \"Discovery Mode\" that prioritizes variety, or by integrating fairness constraints that ensure a mix of mainstream and diverse music in curated playlists. Additionally, more transparency in how recommendations are made would help users better understand and control their listening experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
